{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc02c260",
   "metadata": {},
   "source": [
    "# Evaluating a Pre-trained Model for Semantic Search (Retrieval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9957bf0",
   "metadata": {},
   "source": [
    "## Import libraries and make GPU avaiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0666681c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wei\\Assignment\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import argparse\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Related third-party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Local application/library specific imports\n",
    "from src.data_loader import DataLoader\n",
    "from src.data_preprocessor import DataPreprocessor\n",
    "from src.model_evaluator import ModelEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00deb628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "GPU is available\n",
      "NVIDIA GeForce RTX 4070 SUPER\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available and set device accordingly\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3233f5ab",
   "metadata": {},
   "source": [
    "## Dataset Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff67f84",
   "metadata": {},
   "source": [
    "### ESCI Data \n",
    "\n",
    "The **Shopping Queries Data Set** (https://github.com/amazon-science/esci-data) is a large-scale, manually annotated collection of challenging search queries, created to support research in semantic matching between queries and products. For each query, the dataset includes up to 40 potentially relevant products, each annotated with an ESCI relevance judgment:\n",
    "\n",
    "- **Exact match (E)**\n",
    "- **Substitute (S)**\n",
    "- **Complement (C)**\n",
    "- **Irrelevant (I)**\n",
    "\n",
    "These labels indicate how well each product aligns with the query. Each query-product pair is also accompanied by additional contextual information. The dataset is **multilingual**, featuring queries in **English (us)**, **Japanese (jp)**, and **Spanish (es)**.\n",
    "\n",
    "#### Full Dataset\n",
    "\n",
    "- **130,652** unique queries\n",
    "- **2,621,738** query-item pairs\n",
    "- Provides a broader and more comprehensive set of examples for extensive training and evaluation\n",
    "\n",
    "#### Reduced Version Provided by the Author \n",
    "\n",
    "- **48,300** unique queries\n",
    "- **1,118,011** query-item pairs\n",
    "\n",
    "\n",
    "\n",
    "The dataset is stratified by queries into **train** and **test** splits, ensuring a balanced distribution of query types across both splits.\n",
    "\n",
    "### Data Fields\n",
    "\n",
    "Each entry in the dataset contains the following fields:\n",
    "\n",
    "`example_id`, `query`, `query_id`, `product_id`, `product_locale`, `esci_label`, `small_version`, `large_version`, `split`, `product_title`, `product_description`, `product_bullet_point`, `product_brand`, `product_color`, `source`\n",
    "\n",
    "### Columns Used for This Task\n",
    "\n",
    "For this particular task, we focus on the following columns:\n",
    "\n",
    "- `query`\n",
    "- `product_title`\n",
    "- `product_description`\n",
    "- `product_locale`\n",
    "- `small_version`\n",
    "- `large_version`\n",
    "- `split`\n",
    "- `esci_label`\n",
    "\n",
    "These fields provide the necessary information for training and evaluating semantic search models, combining textual features from queries and product metadata with relevance labels for supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed1a5a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>relevance</th>\n",
       "      <th>split</th>\n",
       "      <th>product_locale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>!awnmower tires without rims</td>\n",
       "      <td>RamPro 10\" All Purpose Utility Air Tires/Wheel...</td>\n",
       "      <td>&lt;b&gt;About The Ram-Pro All Purpose Utility 10\" A...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>!awnmower tires without rims</td>\n",
       "      <td>MaxAuto 2-Pack 13x5.00-6 2PLY Turf Mower Tract...</td>\n",
       "      <td>MaxAuto 2-Pack 13x5.00-6 2PLY Turf Mower Tract...</td>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>!awnmower tires without rims</td>\n",
       "      <td>NEIKO 20601A 14.5 inch Steel Tire Spoon Lever ...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>!awnmower tires without rims</td>\n",
       "      <td>2PK 13x5.00-6 13x5.00x6 13x5x6 13x5-6 2PLY Tur...</td>\n",
       "      <td>Tire Size: 13 x 5.00 - 6 Axle: 3/4\" inside dia...</td>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>!awnmower tires without rims</td>\n",
       "      <td>(Set of 2) 15x6.00-6 Husqvarna/Poulan Tire Whe...</td>\n",
       "      <td>No fuss. Just take off your old assembly and r...</td>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           query  \\\n",
       "16  !awnmower tires without rims   \n",
       "17  !awnmower tires without rims   \n",
       "18  !awnmower tires without rims   \n",
       "19  !awnmower tires without rims   \n",
       "20  !awnmower tires without rims   \n",
       "\n",
       "                                                title  \\\n",
       "16  RamPro 10\" All Purpose Utility Air Tires/Wheel...   \n",
       "17  MaxAuto 2-Pack 13x5.00-6 2PLY Turf Mower Tract...   \n",
       "18  NEIKO 20601A 14.5 inch Steel Tire Spoon Lever ...   \n",
       "19  2PK 13x5.00-6 13x5.00x6 13x5x6 13x5-6 2PLY Tur...   \n",
       "20  (Set of 2) 15x6.00-6 Husqvarna/Poulan Tire Whe...   \n",
       "\n",
       "                                          description  relevance  split  \\\n",
       "16  <b>About The Ram-Pro All Purpose Utility 10\" A...          1  train   \n",
       "17  MaxAuto 2-Pack 13x5.00-6 2PLY Turf Mower Tract...          4  train   \n",
       "18                                               None          1  train   \n",
       "19  Tire Size: 13 x 5.00 - 6 Axle: 3/4\" inside dia...          3  train   \n",
       "20  No fuss. Just take off your old assembly and r...          4  train   \n",
       "\n",
       "   product_locale  \n",
       "16             us  \n",
       "17             us  \n",
       "18             us  \n",
       "19             us  \n",
       "20             us  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the ESCI dataset\n",
    "loaded_data = DataLoader(\n",
    "        example_path=\"dataset/shopping_queries_dataset_examples.parquet\",\n",
    "        products_path=\"dataset/shopping_queries_dataset_products.parquet\",\n",
    "        small_version=True, # Load a small version of the dataset for data exploration\n",
    "    )\n",
    "loaded_data.load_data()\n",
    "loaded_data.df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635aa028",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "In preparation for semantic search and product retrieval tasks, I performed a comprehensive data cleaning process on the **Shopping Queries Data Set**. This was necessary to ensure that the input text is clean, meaningful, and suitable for downstream modeling.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Data Cleaning Steps\n",
    "\n",
    "##### 1. Removal of HTML Tags\n",
    "\n",
    "Upon inspecting the extracted vocabulary, it became clear that a significant portion of the words were HTML tags rather than meaningful content. These tags introduce noise into both bag-of-words models and semantic search embeddings.\n",
    "\n",
    "To address this, I systematically removed all HTML tags from the following text fields:\n",
    "- **query**\n",
    "- **product_title**\n",
    "- **product_description**\n",
    "\n",
    "This ensured that only the true textual content was retained for model training and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "##### 2. Removal of Special Characters\n",
    "\n",
    "For English-language (`us`) data, I further cleaned the text by removing special characters and symbols that do not contribute to the semantic meaning of the text. This included punctuation marks, brackets, and other non-alphanumeric characters, while preserving spaces to maintain word boundaries.\n",
    "\n",
    "This step helped reduce vocabulary fragmentation and improved the quality of both traditional tokenization and semantic embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "##### 3. Text Normalization\n",
    "\n",
    "As part of the cleaning process:\n",
    "- All text was converted to **lowercase** to ensure case-insensitive matching.\n",
    "- Tokenization steps (for bag-of-words analysis) were performed on clean, normalized text.\n",
    "\n",
    "---\n",
    "\n",
    "##### 4. Locale Filtering\n",
    "\n",
    "Since the dataset is multilingual (English, Japanese, Spanish), I filtered the data to focus only on the desired locale (e.g., English `\"us\"`) when necessary.  \n",
    "Different languages may require different tokenization and preprocessing strategies, so this filtering step ensured consistent and comparable training and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "##### 5. Creation of Combined Text Field\n",
    "\n",
    "For feature extraction, I created a new field, **`combined_text`**, by concatenating the cleaned `product_title` and `product_description`.  \n",
    "This allowed the models to leverage richer product information and better capture the full semantic context during training and inference.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58b257a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     word  frequency\n",
      "18     br    4056430\n",
      "48     la     438325\n",
      "66   para     414585\n",
      "30     el     331113\n",
      "53     li     321154\n",
      "..    ...        ...\n",
      "6      18      46060\n",
      "13   baby      46022\n",
      "95  waist      45831\n",
      "81   skin      45089\n",
      "45   just      44797\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Run preprocessing on the loaded data\n",
    "data_preprocessor = DataPreprocessor(loaded_data.df,language=\"us\")\n",
    "print(data_preprocessor.get_bag_of_words(top_n=10))\n",
    "\n",
    "# Clean the data\n",
    "data_preprocessor.data_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce663e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data analysis on products title and description\n",
    "def create_bag_of_words_analysis(df_products):\n",
    "    # Filter for US products\n",
    "    df_products_us = df_products[df_products['product_locale'] == 'us']\n",
    "    \n",
    "    print(f\"Analyzing {len(df_products_us)} US products\")\n",
    "    \n",
    "    # Clean title and description columns\n",
    "    df_products_us['clean_title'] = df_products_us['product_title'].apply(remove_special_characters)\n",
    "    df_products_us['clean_description'] = df_products_us['product_description'].apply(remove_special_characters)\n",
    "    \n",
    "    # Combine title and description for analysis\n",
    "    df_products_us['combined_text'] = df_products_us['clean_title'] + \" \" + df_products_us['clean_description']\n",
    "    \n",
    "    # Create bag of words separately for titles, descriptions, and combined\n",
    "    print(\"\\n===== BAG OF WORDS ANALYSIS =====\")\n",
    "    \n",
    "    # Function to create and analyze bag of words\n",
    "    def analyze_bow(text_series, name, min_df=2, max_features=100):\n",
    "        print(f\"\\n--- {name} Analysis ---\")\n",
    "        \n",
    "        # Remove empty texts\n",
    "        valid_texts = text_series[text_series != \"\"].tolist()\n",
    "        \n",
    "        if not valid_texts:\n",
    "            print(f\"No valid texts found for {name}\")\n",
    "            return None\n",
    "        \n",
    "        # Create bag of words\n",
    "        vectorizer = CountVectorizer(stop_words='english', min_df=min_df, max_features=max_features)\n",
    "        X = vectorizer.fit_transform(valid_texts)\n",
    "        \n",
    "        # Get feature names and counts\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        counts = X.sum(axis=0).A1\n",
    "        \n",
    "        # Create DataFrame with word frequencies\n",
    "        word_freq = pd.DataFrame({'word': feature_names, 'frequency': counts})\n",
    "        word_freq = word_freq.sort_values('frequency', ascending=False)\n",
    "        \n",
    "        print(f\"Top 20 most frequent words for {name}:\")\n",
    "        print(word_freq.head(20))\n",
    "        \n",
    "        # Generate word cloud\n",
    "        create_wordcloud(word_freq, name)\n",
    "        \n",
    "        return word_freq\n",
    "    \n",
    "    # Function to create word cloud\n",
    "    def create_wordcloud(word_freq_df, name):\n",
    "        word_dict = dict(zip(word_freq_df['word'], word_freq_df['frequency']))\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white', \n",
    "                             max_words=100, contour_width=3, contour_color='steelblue')\n",
    "        wordcloud.generate_from_frequencies(word_dict)\n",
    "        \n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(f'Word Cloud - {name}')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        print(f\"Word cloud saved as 'wordcloud_{name.lower().replace(' ', '_')}.png'\")\n",
    "    \n",
    "    # Analyze titles\n",
    "    title_bow = analyze_bow(df_products_us['clean_title'], 'Product Titles')\n",
    "    \n",
    "    # Analyze descriptions\n",
    "    desc_bow = analyze_bow(df_products_us['clean_description'], 'Product Descriptions')\n",
    "    \n",
    "    # Analyze combined text\n",
    "    combined_bow = analyze_bow(df_products_us['combined_text'], 'Combined Title & Description')\n",
    "    \n",
    "    # Create bar plots for top words\n",
    "    def plot_top_words(word_freq_df, name, n=15):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        top_words = word_freq_df.head(n)\n",
    "        sns.barplot(x='frequency', y='word', data=top_words)\n",
    "        plt.title(f'Top {n} Words in {name}')\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    # Plot bar charts for top words\n",
    "    if title_bow is not None:\n",
    "        plot_top_words(title_bow, 'Product Titles')\n",
    "    \n",
    "    if desc_bow is not None:\n",
    "        plot_top_words(desc_bow, 'Product Descriptions')\n",
    "    \n",
    "    if combined_bow is not None:\n",
    "        plot_top_words(combined_bow, 'Combined Text')\n",
    "    \n",
    "    # Return the dataframes for further analysis\n",
    "    return {\n",
    "        'us_products': df_products_us,\n",
    "        'title_bow': title_bow,\n",
    "        'description_bow': desc_bow,\n",
    "        'combined_bow': combined_bow\n",
    "    }\n",
    "\n",
    "products_results = create_bag_of_words_analysis(df_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539839a9",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ddb84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self, language=\"us\", small_version=True):\n",
    "        \"\"\"\n",
    "        Initialize the data preprocessor\n",
    "        \"\"\"\n",
    "        self.language = language\n",
    "        self.small_version = small_version\n",
    "        self.df = None\n",
    "    \n",
    "    def load_data(self, examples_path=\"dataset/shopping_queries_dataset_examples.parquet\", \n",
    "                  products_path=\"dataset/shopping_queries_dataset_products.parquet\"):\n",
    "        \"\"\"\n",
    "        Load the datasets from the specified paths.\n",
    "        \"\"\"\n",
    "        df_examples = pd.read_parquet(examples_path)\n",
    "        df_products = pd.read_parquet(products_path)\n",
    "\n",
    "        # Merge the examples and products dataframes on product_locale and product_id\n",
    "        self.df = pd.merge(\n",
    "            df_examples,\n",
    "            df_products,\n",
    "            how='left',\n",
    "            left_on=['product_locale','product_id'],\n",
    "            right_on=['product_locale', 'product_id']\n",
    "        )\n",
    "\n",
    "        # Create a dictionary to map product locales to their respective relevance scores\n",
    "        esci_dict = {\"E\":4,\"S\":3,\"C\":2,\"I\":1}\n",
    "        self.df['esci_label'] = self.df['esci_label'].map(esci_dict)\n",
    "\n",
    "        # Rename the columns for clarity\n",
    "        self.df.rename(columns={\n",
    "            'esci_label': 'relevance',\n",
    "            'product_title': 'title',\n",
    "            'product_description': 'description',\n",
    "        }, inplace=True)\n",
    "\n",
    "        # Drop na values in the 'revelance' column\n",
    "        self.df = self.df.dropna(subset=['relevance','title','query'])\n",
    "\n",
    "        if self.small_version:\n",
    "            # Create a small version of the dataset for testing purposes\n",
    "            self.df = self.df[self.df['small_version'] == 1]\n",
    "\n",
    "        # Only keep the relevant columns\n",
    "        self.df = self.df[['query', 'title', 'description', 'relevance','split','product_locale']]\n",
    "    \n",
    "    def get_dataset_stats(self):\n",
    "        \"\"\"\n",
    "        Get statistics about the dataset\n",
    "        \"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"Dataset not loaded. Call load_data() first.\")\n",
    "            \n",
    "        stats = {\n",
    "            'total_samples': len(self.df),\n",
    "            'unique_queries': self.df['query'].nunique(),\n",
    "            'avg_products_per_query': len(self.df) / self.df['query'].nunique(),\n",
    "            'relevance_distribution': self.df['relevance'].value_counts(normalize=True).sort_index().to_dict(),\n",
    "        }\n",
    "        \n",
    "        if 'product_locale' in self.df.columns:\n",
    "            stats['locale_distribution'] = self.df['product_locale'].value_counts(normalize=True).to_dict()\n",
    "        \n",
    "        return stats\n",
    "    def split_data(self,split_column):\n",
    "        \"\"\"\n",
    "        Splits the DataFrame into train, validation, and test sets based on the split_column.\n",
    "        \"\"\"\n",
    "        df_train = self.df[self.df[split_column] == 'train']\n",
    "        df_test = self.df[self.df[split_column] == 'test']\n",
    "\n",
    "        print(f\"Train set size: {len(df_train)} rows, {df_train['query'].nunique()} unique queries\")\n",
    "        print(f\"Test set size: {len(df_test)} rows, {df_test['query'].nunique()} unique queries\")\n",
    "        return df_train, df_test\n",
    "    \n",
    "    def data_preprocessing(self):\n",
    "        \"\"\"\n",
    "        Preprocess the data by removing HTML tags and cleaning text.\n",
    "        \"\"\"\n",
    "        # Remove HTML tags and clean text\n",
    "        self.df['query'] = self.df['query'].apply(remove_html_tags)\n",
    "        self.df['title'] = self.df['title'].apply(remove_html_tags)\n",
    "        self.df['description'] = self.df['description'].apply(remove_html_tags)\n",
    "\n",
    "        # Only remove special characters for US language\n",
    "        if (self.language=='us'):\n",
    "            self.df['query'] = self.df['query'].apply(remove_special_characters)\n",
    "            self.df['title'] = self.df['title'].apply(remove_special_characters)\n",
    "            self.df['description'] = self.df['description'].apply(remove_special_characters)\n",
    "\n",
    "        self.df['combined_text'] = self.df['title'] + \" \" + self.df['description']\n",
    "\n",
    "    def filter_by_locale(self):\n",
    "        \"\"\"\n",
    "        Filter the dataset by product locale\n",
    "        \"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"Dataset not loaded. Call load_data() first.\")\n",
    "            \n",
    "        if 'product_locale' not in self.df.columns:\n",
    "            raise ValueError(\"'product_locale' column not found in dataset\")\n",
    "            \n",
    "        filtered_df = self.df[self.df['product_locale'] == self.language]\n",
    "        print(f\"Filtered dataset to {len(filtered_df)} rows with locale '{self.language}'\")\n",
    "        \n",
    "        # Set the filtered DataFrame as the current DataFrame\n",
    "        self.df = filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "407b17f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\", use_gpu=True):\n",
    "        \"\"\"\n",
    "        Initialize the model evaluator\n",
    "        \"\"\"\n",
    "        # Check if CUDA is available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        print(f\"Loaded model: {model_name}\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def calculate_ndcg(relevance_scores, k=10):\n",
    "        \"\"\"\n",
    "        Calculate NDCG@k for a single query\n",
    "        \"\"\"\n",
    "        if len(relevance_scores) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Limit to top k scores\n",
    "        relevance_scores = relevance_scores[:k]\n",
    "        \n",
    "        # Calculate DCG\n",
    "        dcg = 0\n",
    "        for i, score in enumerate(relevance_scores):\n",
    "            dcg += (2 ** score - 1) / np.log2(i + 2)  # i+2 because i is 0-indexed\n",
    "        \n",
    "        # Calculate ideal DCG\n",
    "        ideal_scores = sorted(relevance_scores, reverse=True)\n",
    "        idcg = 0\n",
    "        for i, score in enumerate(ideal_scores):\n",
    "            idcg += (2 ** score - 1) / np.log2(i + 2)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if idcg == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Return NDCG\n",
    "        return dcg / idcg\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_recall_at_k(relevant_items, retrieved_items, k=10):\n",
    "        \"\"\"\n",
    "        Calculate Recall@k for a single query\n",
    "        \"\"\"\n",
    "        if len(relevant_items) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Limit to top k retrieved items\n",
    "        retrieved_items_at_k = set(retrieved_items[:k])\n",
    "        \n",
    "        # Calculate recall\n",
    "        return len(retrieved_items_at_k.intersection(relevant_items)) / len(relevant_items)\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_mrr(retrieved_items, relevant_items, k=10):\n",
    "        \"\"\"\n",
    "        Calculate MRR@k for a single query\n",
    "        \"\"\"\n",
    "        # Limit to top k retrieved items\n",
    "        retrieved_items = retrieved_items[:k]\n",
    "        \n",
    "        # Find the first relevant item\n",
    "        for i, item in enumerate(retrieved_items):\n",
    "            if item in relevant_items:\n",
    "                return 1.0 / (i + 1)  # +1 for 1-based rank\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    def evaluate(self, test_df, threshold=3.0, k=10, batch_size=128):\n",
    "        \"\"\"\n",
    "        Evaluate the model on the test set\n",
    "        \"\"\"\n",
    "        # Group the test data by query\n",
    "        query_groups = test_df.groupby('query')\n",
    "\n",
    "        # Adjust batch size based on GPU memory\n",
    "        if self.device.type == 'cuda':\n",
    "            # Smaller batch size for GPU to avoid out-of-memory errors\n",
    "            if batch_size > 128:\n",
    "                print(f\"Reducing batch size from {batch_size} to 128 for GPU processing\")\n",
    "                batch_size = 128\n",
    "        \n",
    "        # Track metrics for each query\n",
    "        ndcg_scores = []\n",
    "        recall_scores = []\n",
    "        mrr_scores = []\n",
    "        \n",
    "        # Process queries in batches\n",
    "        unique_queries = list(query_groups.groups.keys())\n",
    "        \n",
    "        for i in tqdm(range(0, len(unique_queries), batch_size), desc=\"Evaluating queries\"):\n",
    "            batch_queries = unique_queries[i:i+batch_size]\n",
    "            \n",
    "            # Collect all products for the batch queries\n",
    "            batch_data = []\n",
    "            for query in batch_queries:\n",
    "                query_df = query_groups.get_group(query)\n",
    "                query_products = query_df['combined_text'].tolist()\n",
    "                query_relevances = query_df['relevance'].tolist()\n",
    "                batch_data.append((query, query_products, query_relevances))\n",
    "            \n",
    "            # Encode all unique queries in the batch\n",
    "            query_texts = [item[0] for item in batch_data]\n",
    "\n",
    "            query_embeddings = self.model.encode(\n",
    "                query_texts, \n",
    "                convert_to_tensor=True, \n",
    "                show_progress_bar=False,\n",
    "                device=self.device\n",
    "            )\n",
    "            \n",
    "            # Process each query in the batch\n",
    "            for idx, (query, products, relevances) in enumerate(batch_data):\n",
    "                # Skip if no products\n",
    "                if len(products) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Encode products\n",
    "                product_embeddings = self.model.encode(products, convert_to_tensor=True, show_progress_bar=False\n",
    "                ,device=self.device)\n",
    "                \n",
    "                # Calculate similarities\n",
    "                q_embedding = query_embeddings[idx].unsqueeze(0)  # Add batch dimension\n",
    "                similarities = util.pytorch_cos_sim(q_embedding, product_embeddings)[0].cpu().numpy()\n",
    "                \n",
    "                # Sort products by similarity\n",
    "                sorted_indices = np.argsort(-similarities)\n",
    "                \n",
    "                # Get sorted relevance scores\n",
    "                sorted_relevances = [relevances[idx] for idx in sorted_indices]\n",
    "                \n",
    "                # Calculate NDCG@k\n",
    "                ndcg = self.calculate_ndcg(sorted_relevances, k)\n",
    "                ndcg_scores.append(ndcg)\n",
    "                \n",
    "                # Calculate Recall@k and MRR@k\n",
    "                relevant_indices = {i for i, rel in enumerate(relevances) if rel >= threshold}\n",
    "                retrieved_indices = sorted_indices.tolist()\n",
    "                \n",
    "                recall = self.calculate_recall_at_k(relevant_indices, retrieved_indices, k)\n",
    "                recall_scores.append(recall)\n",
    "                \n",
    "                mrr = self.calculate_mrr(retrieved_indices, relevant_indices, k)\n",
    "                mrr_scores.append(mrr)\n",
    "                if self.device.type == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_ndcg = np.mean(ndcg_scores)\n",
    "        avg_recall = np.mean(recall_scores)\n",
    "        avg_mrr = np.mean(mrr_scores)\n",
    "        \n",
    "        # Return metrics\n",
    "        return {\n",
    "            'NDCG@10': avg_ndcg,\n",
    "            'Recall@10': avg_recall,\n",
    "            'MRR@10': avg_mrr,\n",
    "            'Number of Queries': len(unique_queries)\n",
    "        }\n",
    "    def print_results(self,results,language=None,small_version=None):\n",
    "        \"\"\"\n",
    "        Print evaluation results\n",
    "        \"\"\"\n",
    "        print(\"\\nEvaluation Results:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Model: {self.model_name}\")\n",
    "        print(f\"Dataset: ESCI {language.upper() if language else 'All Languages'} {'Small' if small_version else 'Large'}\")\n",
    "        print(f\"Number of test queries: {results['Number of Queries']}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"NDCG@10: {results['NDCG@10']:.4f}\")\n",
    "        print(f\"Recall@10: {results['Recall@10']:.4f}\")\n",
    "        print(f\"MRR@10: {results['MRR@10']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4cedd5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(language=None, model_name=\"all-MiniLM-L6-v2\", small_version=True,split_column='split',batch_size=64):\n",
    "    \"\"\"\n",
    "    Run the complete evaluation pipeline\n",
    "    \"\"\"\n",
    "    # Initialize data preprocessor\n",
    "    if language is None:\n",
    "        print(f\"Initializing data preprocessor for all languages\")\n",
    "    else:\n",
    "        print(f\"Initializing data preprocessor for language: {language}\")\n",
    "    preprocessor = DataPreprocessor(language=language, small_version=small_version)\n",
    "    \n",
    "    # Download and prepare dataset\n",
    "    preprocessor.load_data()\n",
    "    \n",
    "    # Preprocess data\n",
    "    preprocessor.data_preprocessing()\n",
    "\n",
    "    # Filter by locale\n",
    "    if (language is not None):\n",
    "        preprocessor.filter_by_locale()\n",
    "    \n",
    "    # Get dataset statistics\n",
    "    stats = preprocessor.get_dataset_stats()\n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"Total samples: {stats['total_samples']}\")\n",
    "    print(f\"Unique queries: {stats['unique_queries']}\")\n",
    "    print(f\"Average products per query: {stats['avg_products_per_query']:.2f}\")\n",
    "    print(\"Relevance distribution:\")\n",
    "    for score, pct in stats['relevance_distribution'].items():\n",
    "        print(f\"  Score {score}: {pct:.2%}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_df, test_df = preprocessor.split_data(split_column)\n",
    "    \n",
    "    # Initialize model evaluator\n",
    "    print(f\"\\nInitializing model evaluator with model: {model_name}\")\n",
    "    evaluator = ModelEvaluator(model_name=model_name)\n",
    "    \n",
    "    # Evaluate test model\n",
    "    print(f\"Evaluating model on {language} test set...\")\n",
    "    results = evaluator.evaluate(test_df, threshold=3.0, batch_size=batch_size)  # Consider 'E' and 'S' as relevant (4 and 3)\n",
    "    evaluator.print_results(results)\n",
    "\n",
    "    # Evaluate train model\n",
    "    print(f\"\\nEvaluating model on {language} train set...\")\n",
    "    results = evaluator.evaluate(train_df, threshold=3.0, batch_size=batch_size)  # Consider 'E' and 'S' as relevant (4 and 3)\n",
    "    evaluator.print_results(results)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e4d3091",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30346050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing data preprocessor for language: us\n",
      "Filtered dataset to 601354 rows with locale 'us'\n",
      "\n",
      "Dataset Statistics:\n",
      "Total samples: 601354\n",
      "Unique queries: 29842\n",
      "Average products per query: 20.15\n",
      "Relevance distribution:\n",
      "  Score 1: 16.87%\n",
      "  Score 2: 4.52%\n",
      "  Score 3: 35.12%\n",
      "  Score 4: 43.49%\n",
      "Train set size: 419653 rows, 20887 unique queries\n",
      "Test set size: 181701 rows, 8955 unique queries\n",
      "\n",
      "Initializing model evaluator with model: all-MiniLM-L6-v2\n",
      "Using device: cuda\n",
      "Loaded model: all-MiniLM-L6-v2\n",
      "Evaluating model on us test set...\n",
      "Reducing batch size from 128 to 64 for GPU processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 140/140 [02:43<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "--------------------------------------------------\n",
      "Model: all-MiniLM-L6-v2\n",
      "Dataset: ESCI All Languages Large\n",
      "Number of test queries: 8955\n",
      "--------------------------------------------------\n",
      "NDCG@10: 0.9072\n",
      "Recall@10: 0.6034\n",
      "MRR@10: 0.9244\n",
      "\n",
      "Evaluating model on us train set...\n",
      "Reducing batch size from 128 to 64 for GPU processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 327/327 [09:03<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "--------------------------------------------------\n",
      "Model: all-MiniLM-L6-v2\n",
      "Dataset: ESCI All Languages Large\n",
      "Number of test queries: 20887\n",
      "--------------------------------------------------\n",
      "NDCG@10: 0.9060\n",
      "Recall@10: 0.6077\n",
      "MRR@10: 0.9242\n",
      "\n",
      "\n",
      "Initializing data preprocessor for language: us\n",
      "Filtered dataset to 1818825 rows with locale 'us'\n",
      "\n",
      "Dataset Statistics:\n",
      "Total samples: 1818825\n",
      "Unique queries: 97307\n",
      "Average products per query: 18.69\n",
      "Relevance distribution:\n",
      "  Score 1: 8.90%\n",
      "  Score 2: 2.20%\n",
      "  Score 3: 20.31%\n",
      "  Score 4: 68.59%\n",
      "Train set size: 1393063 rows, 74865 unique queries\n",
      "Test set size: 425762 rows, 22455 unique queries\n",
      "\n",
      "Initializing model evaluator with model: all-MiniLM-L6-v2\n",
      "Using device: cuda\n",
      "Loaded model: all-MiniLM-L6-v2\n",
      "Evaluating model on us test set...\n",
      "Reducing batch size from 128 to 64 for GPU processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 351/351 [09:06<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "--------------------------------------------------\n",
      "Model: all-MiniLM-L6-v2\n",
      "Dataset: ESCI All Languages Large\n",
      "Number of test queries: 22455\n",
      "--------------------------------------------------\n",
      "NDCG@10: 0.9556\n",
      "Recall@10: 0.6136\n",
      "MRR@10: 0.9659\n",
      "\n",
      "Evaluating model on us train set...\n",
      "Reducing batch size from 128 to 64 for GPU processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 1170/1170 [28:43<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "--------------------------------------------------\n",
      "Model: all-MiniLM-L6-v2\n",
      "Dataset: ESCI All Languages Large\n",
      "Number of test queries: 74865\n",
      "--------------------------------------------------\n",
      "NDCG@10: 0.9652\n",
      "Recall@10: 0.6177\n",
      "MRR@10: 0.9743\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation for US language with small and full dataset versions\n",
    "us_results_small = run_evaluation(language=\"us\", model_name=\"all-MiniLM-L6-v2\", small_version=True,split_column='split',batch_size=64)\n",
    "us_results_full = run_evaluation(language=\"us\", model_name=\"all-MiniLM-L6-v2\", small_version=False,split_column='split',batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f758377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing data preprocessor for all languages\n",
      "\n",
      "Dataset Statistics:\n",
      "Total samples: 1118011\n",
      "Unique queries: 48249\n",
      "Average products per query: 23.17\n",
      "Relevance distribution:\n",
      "  Score 1: 16.76%\n",
      "  Score 2: 5.15%\n",
      "  Score 3: 34.30%\n",
      "  Score 4: 43.78%\n",
      "Train set size: 781638 rows, 33776 unique queries\n",
      "Test set size: 336373 rows, 14488 unique queries\n",
      "\n",
      "Initializing model evaluator with model: all-MiniLM-L6-v2\n",
      "Using device: cuda\n",
      "Loaded model: all-MiniLM-L6-v2\n",
      "Evaluating model on None test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 114/114 [08:36<00:00,  4.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "--------------------------------------------------\n",
      "Model: all-MiniLM-L6-v2\n",
      "Dataset: ESCI All Languages Large\n",
      "Number of test queries: 14488\n",
      "--------------------------------------------------\n",
      "NDCG@10: 0.9027\n",
      "Recall@10: 0.5502\n",
      "MRR@10: 0.9147\n",
      "\n",
      "Evaluating model on None train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries:  83%|████████▎ | 219/264 [13:49<03:21,  4.48s/it]"
     ]
    }
   ],
   "source": [
    "# Run evaluation for all langauges (us, jp, es) with small and full dataset versions\n",
    "us_results_small = run_evaluation(model_name=\"all-MiniLM-L6-v2\", small_version=True,split_column='split',batch_size=128)\n",
    "us_results_full = run_evaluation(model_name=\"all-MiniLM-L6-v2\", small_version=False,split_column='split',batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db815370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea0aa3cf",
   "metadata": {},
   "source": [
    "## Limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1bac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langdetect import detect\n",
    "\n",
    "# def detect_language(text):\n",
    "#     try:\n",
    "#         return detect(text)\n",
    "#     except:\n",
    "#         return 'unknown'\n",
    "\n",
    "# # Add language detection to preprocessing\n",
    "# df_examples_products_small['query_lang'] = df_examples_products_small['query'].apply(detect_language)\n",
    "# df_examples_products_small['title_lang'] = df_examples_products_small['title'].apply(detect_language)\n",
    "# df_examples_products_small['description_lang'] = df_examples_products_small['description'].apply(detect_language)\n",
    "\n",
    "# # Create language match flags\n",
    "# df_examples_products_small['query_title_same_lang'] = df_examples_products_small['query_lang'] == df_examples_products_small['title_lang']\n",
    "\n",
    "# # Analyze language distribution\n",
    "# print(df_examples_products_small['query_lang'].value_counts())\n",
    "# print(df_examples_products_small['title_lang'].value_counts())\n",
    "# print(df_examples_products_small.groupby(['query_lang', 'title_lang']).size())\n",
    "\n",
    "# # Evaluate separately by language match\n",
    "# english_matches = df_examples_products_small[(df_examples_products_small['query_lang'] == 'en') & (df_examples_products_small['title_lang'] == 'en')]\n",
    "# non_english_matches = df_examples_products_small[(df_examples_products_small['query_lang'] != 'en') & (df_examples_products_small['query_lang'] == df_examples_products_small['title_lang'])]\n",
    "# cross_language = df_examples_products_small[df_examples_products_small['query_lang'] != df_examples_products_small['title_lang']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
